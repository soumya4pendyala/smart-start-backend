{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac0f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e330c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages : Annotated[list, add_messages]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c136855",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbffb230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe55af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm= ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f5fece1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F26F3F0D70>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F26F3F1940>, model_name='llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c9149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state:State):\n",
    "    return {\"messages\":[llm.invoke(state[\"messages\"])]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c05efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"basicChatbot\",chatbot)\n",
    "graph_builder.add_edge(START,\"basicChatbot\")\n",
    "graph_builder.add_edge(\"basicChatbot\",END)\n",
    "graph=graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image,display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b454c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=graph.invoke({\"messages\":\"Hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aef702",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "766665cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"messages\":\"Provide me the recent AI news\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de6077e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is langgraph?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
       "   'title': 'LangGraph Tutorial: What Is LangGraph and How to Use It?',\n",
       "   'content': 'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions.',\n",
       "   'score': 0.9520821,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.ibm.com/think/topics/langgraph',\n",
       "   'title': 'What is LangGraph? - IBM',\n",
       "   'content': 'LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agentâ€™s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. Nodes: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.',\n",
       "   'score': 0.94584644,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 1.18}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "tool.invoke(\"What is langgraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7504031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a:int,b:int)->int:\n",
    "    \"\"\"Multiply a and b\n",
    "    \n",
    "    Args:\n",
    "        a (int): first int\n",
    "        b (int): second int\n",
    "\n",
    "    Returns:\n",
    "        int : output int\n",
    "    \"\"\"\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf6bbcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command, interrupt\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def human_assistance(query : str) -> str:\n",
    "    \"\"\" Request assistance from human \"\"\"\n",
    "    human_response = interrupt({\"query\": query})\n",
    "    return human_response[\"data\"]\n",
    "\n",
    "tools=[tool,human_assistance]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de99d6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function langchain_core.tools.convert.tool(name_or_callable: Union[str, Callable, NoneType] = None, runnable: Optional[langchain_core.runnables.base.Runnable] = None, *args: Any, description: Optional[str] = None, return_direct: bool = False, args_schema: Union[type[pydantic.main.BaseModel], dict[str, Any], NoneType] = None, infer_schema: bool = True, response_format: Literal['content', 'content_and_artifact'] = 'content', parse_docstring: bool = False, error_on_invalid_docstring: bool = True) -> Union[langchain_core.tools.base.BaseTool, Callable[[Union[Callable, langchain_core.runnables.base.Runnable]], langchain_core.tools.base.BaseTool]]>,\n",
       " StructuredTool(name='human_assistance', description='Request assistance from human', args_schema=<class 'langchain_core.utils.pydantic.human_assistance'>, func=<function human_assistance at 0x000001F26F10CC20>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87ad1bef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Arg Precedence for the tool description value is as follows in docstring not found in function signature.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m llm_with_tools=\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramu\\hackathon\\.venv\\Lib\\site-packages\\langchain_groq\\chat_models.py:842\u001b[39m, in \u001b[36mChatGroq.bind_tools\u001b[39m\u001b[34m(self, tools, tool_choice, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind_tools\u001b[39m(\n\u001b[32m    818\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    819\u001b[39m     tools: Sequence[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mtype\u001b[39m[BaseModel], Callable, BaseTool]],\n\u001b[32m   (...)\u001b[39m\u001b[32m    824\u001b[39m     **kwargs: Any,\n\u001b[32m    825\u001b[39m ) -> Runnable[LanguageModelInput, BaseMessage]:\n\u001b[32m    826\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Bind tool-like objects to this chat model.\u001b[39;00m\n\u001b[32m    827\u001b[39m \n\u001b[32m    828\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    840\u001b[39m \n\u001b[32m    841\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m     formatted_tools = [\u001b[43mconvert_to_openai_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools]\n\u001b[32m    843\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tool_choice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tool_choice:\n\u001b[32m    844\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m tool_choice == \u001b[33m\"\u001b[39m\u001b[33many\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramu\\hackathon\\.venv\\Lib\\site-packages\\langchain_core\\utils\\function_calling.py:584\u001b[39m, in \u001b[36mconvert_to_openai_tool\u001b[39m\u001b[34m(tool, strict)\u001b[39m\n\u001b[32m    582\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (tool.get(\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).startswith(\u001b[33m\"\u001b[39m\u001b[33mweb_search_preview\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    583\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tool\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m oai_function = \u001b[43mconvert_to_openai_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m: oai_function}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramu\\hackathon\\.venv\\Lib\\site-packages\\langchain_core\\utils\\function_calling.py:477\u001b[39m, in \u001b[36mconvert_to_openai_function\u001b[39m\u001b[34m(function, strict)\u001b[39m\n\u001b[32m    474\u001b[39m     oai_function = cast(\u001b[33m\"\u001b[39m\u001b[33mdict\u001b[39m\u001b[33m\"\u001b[39m, _format_tool_to_openai_function(function))\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(function):\n\u001b[32m    476\u001b[39m     oai_function = cast(\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdict\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43m_convert_python_function_to_openai_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     )\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    480\u001b[39m     msg = (\n\u001b[32m    481\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported function\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFunctions must be passed in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m as Dict, pydantic.BaseModel, or Callable. If they\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre a dict they must\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m either be in OpenAI function format or valid JSON schema with top-level\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    484\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keys.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramu\\hackathon\\.venv\\Lib\\site-packages\\langchain_core\\utils\\function_calling.py:223\u001b[39m, in \u001b[36m_convert_python_function_to_openai_function\u001b[39m\u001b[34m(function)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_schema_from_function\n\u001b[32m    222\u001b[39m func_name = _get_python_function_name(function)\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m model = \u001b[43mcreate_schema_from_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_docstring\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_on_invalid_docstring\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_injected\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _convert_pydantic_to_openai_function(\n\u001b[32m    232\u001b[39m     model,\n\u001b[32m    233\u001b[39m     name=func_name,\n\u001b[32m    234\u001b[39m     description=model.\u001b[34m__doc__\u001b[39m,\n\u001b[32m    235\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramu\\hackathon\\.venv\\Lib\\site-packages\\langchain_core\\tools\\base.py:350\u001b[39m, in \u001b[36mcreate_schema_from_function\u001b[39m\u001b[34m(model_name, func, filter_args, parse_docstring, error_on_invalid_docstring, include_injected)\u001b[39m\n\u001b[32m    345\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m include_injected \u001b[38;5;129;01mand\u001b[39;00m _is_injected_arg_type(\n\u001b[32m    346\u001b[39m             sig.parameters[existing_param].annotation\n\u001b[32m    347\u001b[39m         ):\n\u001b[32m    348\u001b[39m             filter_args_.append(existing_param)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m description, arg_descriptions = \u001b[43m_infer_arg_descriptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_docstring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_docstring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_on_invalid_docstring\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_on_invalid_docstring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[38;5;66;03m# Pydantic adds placeholder virtual fields we need to strip\u001b[39;00m\n\u001b[32m    356\u001b[39m valid_properties = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramu\\hackathon\\.venv\\Lib\\site-packages\\langchain_core\\tools\\base.py:208\u001b[39m, in \u001b[36m_infer_arg_descriptions\u001b[39m\u001b[34m(fn, parse_docstring, error_on_invalid_docstring)\u001b[39m\n\u001b[32m    206\u001b[39m     arg_descriptions = {}\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parse_docstring:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[43m_validate_docstring_args_against_annotations\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_descriptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg, arg_type \u001b[38;5;129;01min\u001b[39;00m annotations.items():\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m arg_descriptions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramu\\hackathon\\.venv\\Lib\\site-packages\\langchain_core\\tools\\base.py:180\u001b[39m, in \u001b[36m_validate_docstring_args_against_annotations\u001b[39m\u001b[34m(arg_descriptions, annotations)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m docstring_arg \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m annotations:\n\u001b[32m    179\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mArg \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocstring_arg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in docstring not found in function signature.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Arg Precedence for the tool description value is as follows in docstring not found in function signature."
     ]
    }
   ],
   "source": [
    "llm_with_tools=llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c75c289",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_with_tools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mllm_with_tools\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'llm_with_tools' is not defined"
     ]
    }
   ],
   "source": [
    "llm_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d4f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "def tool_calling_llm(state: State) :\n",
    "    return {\"messages\":[llm_with_tools.invoke(state[\"messages\"])]} \n",
    "\n",
    "\n",
    "builder= StateGraph(State)\n",
    "builder.add_node(\"tool_calling_llm\",tool_calling_llm)\n",
    "builder.add_node(\"tools\",ToolNode(tools))\n",
    "\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\",\n",
    "    #If the latest message from assistant is a tool call, tools_condition routes to tool\n",
    "    #If the latest message from assistant is not a tool call, tools_condition routes to END\n",
    "    tools_condition\n",
    ")\n",
    "builder.add_edge(\"tools\",END)\n",
    "\n",
    "graph=builder.compile()\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0290361",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"messages\":\"what is the latest AI News\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e73666",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"messages\":\"what is the latest AI News\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c19999",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c59aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in response[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630edc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"messages\":\"hi, how are u? what is the latest ai news and then 2 multiply by 3 then multiply by 10\"})\n",
    "for m in response[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2369fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'State' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpoint\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MemorySaver\n\u001b[32m      8\u001b[39m memory=MemorySaver()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtool_calling_llm\u001b[39m(state: \u001b[43mState\u001b[49m) :\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m:[llm_with_tools.invoke(state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m])]} \n\u001b[32m     12\u001b[39m builder= StateGraph(State)\n",
      "\u001b[31mNameError\u001b[39m: name 'State' is not defined"
     ]
    }
   ],
   "source": [
    "### ReAct Architecture with memory\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory=MemorySaver()\n",
    "\n",
    "def tool_calling_llm(state: State) :\n",
    "    return {\"messages\":[llm_with_tools.invoke(state[\"messages\"])]} \n",
    "builder= StateGraph(State)\n",
    "builder.add_node(\"tool_calling_llm\",tool_calling_llm)\n",
    "builder.add_node(\"tools\",ToolNode(tools))\n",
    "\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\",\n",
    "    #If the latest message from assistant is a tool call, tools_condition routes to tool\n",
    "    #If the latest message from assistant is not a tool call, tools_condition routes to END\n",
    "    tools_condition\n",
    ")\n",
    "builder.add_edge(\"tools\",\"tool_calling_llm\")\n",
    "\n",
    "graph=builder.compile(checkpointer=memory)\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e49658",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"thread_id\":\"1\"}}\n",
    "response=graph.invoke({\"messages\":\"Hi, do you remember my name?\"},config=config)\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe38554",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"messages\":\"hi, how are u? what is the latest ai news and then 2 multiply by 3 then multiply by 10\"})\n",
    "for m in response[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab80a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def superbot(state:State):\n",
    "    return {\"messages\":[llm.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976fc24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "builder.add_node(\"SuperBot\", superbot)\n",
    "builder.add_edge(START,\"SuperBot\" )\n",
    "builder.add_edge(\"SuperBot\",END )\n",
    "graph_builder=builder.compile(checkpointer=memory)\n",
    "\n",
    "try:\n",
    "    display(Image(graph_builder.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df58ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"thread_id\":\"1\"}}\n",
    "response=graph_builder.invoke({\"messages\":\"Hi, My name is Soumya, I like to dance?\"},config=config)\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf668f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"thread_id\":\"2\"}}\n",
    "for chunk in graph_builder.stream({\"messages\":\"Hi, My name is Soumya, I like to dance?\"},config=config,stream_mode='updates'):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efafd90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"thread_id\":\"4\"}}\n",
    "for chunk in graph_builder.stream({\"messages\":\"Hi, My name is Soumya, I like to dance\"},config=config,stream_mode='values'):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa44f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph_builder.stream({\"messages\":\"I also like to sing\"},config=config,stream_mode='values'):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585c45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "505d2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ReAct Architecture with memory and human feedback\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.types import Command, interrupt\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm= ChatGroq(model=\"llama3-8b-8192\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages : Annotated[list,add_messages]\n",
    "\n",
    "memory=MemorySaver()\n",
    "\n",
    "def tool_calling_llm(state: State) :\n",
    "    return {\"messages\":[llm_with_tools.invoke(state[\"messages\"])]} \n",
    "\n",
    "new_builder= StateGraph(State)\n",
    "\n",
    "@tool\n",
    "def human_assistance(query : str) -> str:\n",
    "    \"\"\" Request assistance from human \"\"\"\n",
    "    human_response = interrupt({\"query\": query})\n",
    "    return human_response[\"data\"]\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool,human_assistance]\n",
    "llm_with_new_tools=llm.bind_tools(tools)\n",
    "\n",
    "def chat_bot(state: State) :\n",
    "    message = llm_with_new_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\" : [message]}\n",
    "\n",
    "new_builder.add_node(\"chat_bot\",chat_bot)\n",
    "new_builder.add_node(\"tools\",ToolNode(tools))\n",
    "\n",
    "new_builder.add_edge(START, \"chat_bot\")\n",
    "new_builder.add_conditional_edges(\n",
    "    \"chat_bot\",\n",
    "    #If the latest message from assistant is a tool call, tools_condition routes to new_tools\n",
    "    #If the latest message from assistant is not a tool call, tools_condition routes to END\n",
    "    tools_condition\n",
    ")\n",
    "new_builder.add_edge(\"tools\",\"chat_bot\")\n",
    "\n",
    "graph_builder_new=new_builder.compile(checkpointer=memory)\n",
    "\n",
    "try:\n",
    "    display(Image(graph_builder_new.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2056d498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='I need some expert guidance for building an AI Agent. Could you request assistance for me?', additional_kwargs={}, response_metadata={}, id='12f4d2b8-650b-4b91-9612-208e66c94967')]}\n",
      "{'messages': [HumanMessage(content='I need some expert guidance for building an AI Agent. Could you request assistance for me?', additional_kwargs={}, response_metadata={}, id='12f4d2b8-650b-4b91-9612-208e66c94967'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'j0dnypwxr', 'function': {'arguments': '{\"query\":\"I need some expert guidance for building an AI Agent. Could you request assistance for me?\"}', 'name': 'human_assistance'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 2885, 'total_tokens': 2978, 'completion_time': 0.154827305, 'prompt_time': 0.380573049, 'queue_time': 0.7159117460000001, 'total_time': 0.535400354}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_5b339000ab', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--064be175-b37d-4e13-baf7-335b90dc7220-0', tool_calls=[{'name': 'human_assistance', 'args': {'query': 'I need some expert guidance for building an AI Agent. Could you request assistance for me?'}, 'id': 'j0dnypwxr', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2885, 'output_tokens': 93, 'total_tokens': 2978})]}\n"
     ]
    }
   ],
   "source": [
    "user_input = \"I need some expert guidance for building an AI Agent. Could you request assistance for me?\"\n",
    "config={\"configurable\":{\"thread_id\":\"1\"}}\n",
    "for chunk in graph_builder_new.stream({\"messages\":user_input},config=config,stream_mode='values'):\n",
    "    print(chunk)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72388aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_response = (\n",
    "    \"We are here to help. Please checkout LangGraph\"\n",
    "    \"its reliable and extendable\"\n",
    ")\n",
    "\n",
    "human_command = Command(resume={\"data\": human_response})\n",
    "\n",
    "for chunk in graph_builder_new.stream(human_command,config=config,stream_mode=\"values\"):\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "627cea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running the graph (should interrupt at the human_assistance tool) ---\n",
      "Graph Output: {'messages': [HumanMessage(content='I need some expert guidance for building an AI Agent. Could you request assistance for me?', additional_kwargs={}, response_metadata={}, id='12f4d2b8-650b-4b91-9612-208e66c94967'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'j0dnypwxr', 'function': {'arguments': '{\"query\":\"I need some expert guidance for building an AI Agent. Could you request assistance for me?\"}', 'name': 'human_assistance'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 2885, 'total_tokens': 2978, 'completion_time': 0.154827305, 'prompt_time': 0.380573049, 'queue_time': 0.7159117460000001, 'total_time': 0.535400354}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_5b339000ab', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--064be175-b37d-4e13-baf7-335b90dc7220-0', tool_calls=[{'name': 'human_assistance', 'args': {'query': 'I need some expert guidance for building an AI Agent. Could you request assistance for me?'}, 'id': 'j0dnypwxr', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2885, 'output_tokens': 93, 'total_tokens': 2978}), HumanMessage(content='I need some expert guidance for building an AI Agent. Could you request assistance for me?', additional_kwargs={}, response_metadata={}, id='71561a1c-4bb9-44f6-98f5-6836d4ad7cb4'), AIMessage(content=\"I'd be happy to help! The topic of building an AI Agent is quite broad, but I'll do my best to provide some general guidance and point you in the right direction.\\n\\nTo start, there are many types of AI Agents, each with their own strengths and weaknesses. Some common types include:\\n\\n1. Rule-Based Agents: These agents use pre-defined rules to make decisions and take actions.\\n2. Machine Learning Agents: These agents use machine learning algorithms to learn from data and make predictions.\\n3. Hybrid Agents: These agents combine rule-based and machine learning approaches to make decisions.\\n\\nWhen building an AI Agent, it's essential to consider the following:\\n\\n1. Define the problem you're trying to solve: What is the goal of your AI Agent? What tasks do you want it to perform?\\n2. Identify the data you need: What data do you need to train and test your AI Agent? How will you collect and process this data?\\n3. Choose the right algorithms: Which machine learning or rule-based algorithms are best suited for your problem?\\n4. Consider the environment: How will your AI Agent interact with its environment? Will it be a simple simulation or a complex real-world scenario?\\n\\nIf you're just starting out, I recommend exploring some online resources and tutorials to get a better understanding of the basics. There are many excellent courses and tutorials available on platforms like Coursera, edX, and Udemy.\\n\\nAdditionally, you might want to consider seeking guidance from experts in the field. There are many online communities and forums dedicated to AI and machine learning, where you can ask questions and get feedback from experienced professionals.\\n\\nI hope this helps! Do you have any specific questions or areas you'd like me to focus on?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 350, 'prompt_tokens': 2968, 'total_tokens': 3318, 'completion_time': 0.585495427, 'prompt_time': 0.325549904, 'queue_time': 0.26444134999999996, 'total_time': 0.911045331}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_5b339000ab', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--1c8c1922-2efa-4029-97c2-461a3cddcd30-0', usage_metadata={'input_tokens': 2968, 'output_tokens': 350, 'total_tokens': 3318}), HumanMessage(content='I need some expert guidance for building an AI Agent. Could you request assistance for me?', additional_kwargs={}, response_metadata={}, id='20a6365f-b9f2-4ae8-a901-175f31afd1b7')]}\n",
      "Graph Output: {'messages': [HumanMessage(content='I need some expert guidance for building an AI Agent. Could you request assistance for me?', additional_kwargs={}, response_metadata={}, id='12f4d2b8-650b-4b91-9612-208e66c94967'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'j0dnypwxr', 'function': {'arguments': '{\"query\":\"I need some expert guidance for building an AI Agent. Could you request assistance for me?\"}', 'name': 'human_assistance'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 2885, 'total_tokens': 2978, 'completion_time': 0.154827305, 'prompt_time': 0.380573049, 'queue_time': 0.7159117460000001, 'total_time': 0.535400354}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_5b339000ab', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--064be175-b37d-4e13-baf7-335b90dc7220-0', tool_calls=[{'name': 'human_assistance', 'args': {'query': 'I need some expert guidance for building an AI Agent. Could you request assistance for me?'}, 'id': 'j0dnypwxr', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2885, 'output_tokens': 93, 'total_tokens': 2978}), HumanMessage(content='I need some expert guidance for building an AI Agent. Could you request assistance for me?', additional_kwargs={}, response_metadata={}, id='71561a1c-4bb9-44f6-98f5-6836d4ad7cb4'), AIMessage(content=\"I'd be happy to help! The topic of building an AI Agent is quite broad, but I'll do my best to provide some general guidance and point you in the right direction.\\n\\nTo start, there are many types of AI Agents, each with their own strengths and weaknesses. Some common types include:\\n\\n1. Rule-Based Agents: These agents use pre-defined rules to make decisions and take actions.\\n2. Machine Learning Agents: These agents use machine learning algorithms to learn from data and make predictions.\\n3. Hybrid Agents: These agents combine rule-based and machine learning approaches to make decisions.\\n\\nWhen building an AI Agent, it's essential to consider the following:\\n\\n1. Define the problem you're trying to solve: What is the goal of your AI Agent? What tasks do you want it to perform?\\n2. Identify the data you need: What data do you need to train and test your AI Agent? How will you collect and process this data?\\n3. Choose the right algorithms: Which machine learning or rule-based algorithms are best suited for your problem?\\n4. Consider the environment: How will your AI Agent interact with its environment? Will it be a simple simulation or a complex real-world scenario?\\n\\nIf you're just starting out, I recommend exploring some online resources and tutorials to get a better understanding of the basics. There are many excellent courses and tutorials available on platforms like Coursera, edX, and Udemy.\\n\\nAdditionally, you might want to consider seeking guidance from experts in the field. There are many online communities and forums dedicated to AI and machine learning, where you can ask questions and get feedback from experienced professionals.\\n\\nI hope this helps! Do you have any specific questions or areas you'd like me to focus on?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 350, 'prompt_tokens': 2968, 'total_tokens': 3318, 'completion_time': 0.585495427, 'prompt_time': 0.325549904, 'queue_time': 0.26444134999999996, 'total_time': 0.911045331}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_5b339000ab', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--1c8c1922-2efa-4029-97c2-461a3cddcd30-0', usage_metadata={'input_tokens': 2968, 'output_tokens': 350, 'total_tokens': 3318}), HumanMessage(content='I need some expert guidance for building an AI Agent. Could you request assistance for me?', additional_kwargs={}, response_metadata={}, id='20a6365f-b9f2-4ae8-a901-175f31afd1b7'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'n8yhcfajh', 'function': {'arguments': '{\"query\":\"I need some expert guidance for building an AI Agent. Could you request assistance for me?\"}', 'name': 'human_assistance'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 3345, 'total_tokens': 3401, 'completion_time': 0.093328596, 'prompt_time': 0.507797614, 'queue_time': 0.913859937, 'total_time': 0.60112621}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_5b339000ab', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--e3fa6bf8-22c3-488f-a919-ee0edd621d15-0', tool_calls=[{'name': 'human_assistance', 'args': {'query': 'I need some expert guidance for building an AI Agent. Could you request assistance for me?'}, 'id': 'n8yhcfajh', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3345, 'output_tokens': 56, 'total_tokens': 3401})]}\n",
      "--- The graph is now interrupted. It is waiting for the human command. ---\n",
      "--- Sending the human command to resume the graph ---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  human_assistance (n8yhcfajh)\n",
      " Call ID: n8yhcfajh\n",
      "  Args:\n",
      "    query: I need some expert guidance for building an AI Agent. Could you request assistance for me?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: human_assistance\n",
      "\n",
      "We are here to help. Please checkout LangGraphits reliable and extendable\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Thank you for the result of the tool call. It seems that the tool \"human_assistance\" has provided a suggestion for further guidance.\n",
      "\n",
      "According to the result, it appears that LangGraphits is a reliable and extendable resource for building AI Agents. LangGraphits is likely a platform or framework that provides a solid foundation for building natural language processing (NLP) and machine learning models.\n",
      "\n",
      "As a next step, I would recommend exploring the LangGraphits platform to see what features and tools it offers for building AI Agents. You may want to check out their documentation, tutorials, and examples to get a better understanding of how it can help you achieve your goals.\n",
      "\n",
      "Additionally, you may also want to consider reaching out to the LangGraphits community or support team for more specific guidance and advice on how to get started with building your AI Agent.\n",
      "\n",
      "Please let me know if you have any further questions or if there's anything else I can help you with.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "# Your existing setup code...\n",
    "# ... llm, new_builder, etc.\n",
    "# graph_builder_new = new_builder.compile(checkpointer=memory)\n",
    "\n",
    "# Step 1: Run the graph until it encounters the interrupt\n",
    "user_input = \"I need some expert guidance for building an AI Agent. Could you request assistance for me?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "print(\"--- Running the graph (should interrupt at the human_assistance tool) ---\")\n",
    "try:\n",
    "    for chunk in graph_builder_new.stream({\"messages\": user_input}, config=config, stream_mode='values'):\n",
    "        # This will print the messages as they are generated by the graph\n",
    "        # and the stream will stop when it yields the Command object.\n",
    "        print(f\"Graph Output: {chunk}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the first stream call: {e}\")\n",
    "    # If an error happens here, the second loop will not be reached.\n",
    "\n",
    "print(\"--- The graph is now interrupted. It is waiting for the human command. ---\")\n",
    "\n",
    "# Step 2: Provide the human response to resume the graph\n",
    "human_response = (\n",
    "    \"We are here to help. Please checkout LangGraph\"\n",
    "    \"its reliable and extendable\"\n",
    ")\n",
    "human_command = Command(resume={\"data\": human_response})\n",
    "\n",
    "print(\"--- Sending the human command to resume the graph ---\")\n",
    "events = graph_builder_new.stream(human_command,config,stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a36eefd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_typing', 'cache', 'channels', 'checkpoint', 'config', 'constants', 'errors', 'graph', 'managed', 'prebuilt', 'pregel', 'store', 'types', 'typing', 'utils', 'warnings']\n"
     ]
    }
   ],
   "source": [
    "# Step A: List available symbols\n",
    "import langgraph\n",
    "print(dir(langgraph))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
